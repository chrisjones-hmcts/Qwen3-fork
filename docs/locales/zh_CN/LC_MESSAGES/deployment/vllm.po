# Copyright (C) 2024, Qwen Team, Alibaba Group.
# This file is distributed under the same license as the Qwen package.
#
msgid ""
msgstr ""
"Project-Id-Version: Qwen \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-04-28 19:42+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: zh_CN\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.17.0\n"

#: ../../Qwen/source/deployment/vllm.md:1 faa6e2bc47c24c6dab7113f73d67b0c4
msgid "vLLM"
msgstr ""

#: ../../Qwen/source/deployment/vllm.md:3 45682d60b2ee469bac6a473f7aacbe38
msgid "We recommend you trying [vLLM](https://github.com/vllm-project/vllm) for your deployment of Qwen.  It is simple to use, and it is fast with state-of-the-art serving throughput, efficient management of attention key value memory with PagedAttention, continuous batching of input requests, optimized CUDA kernels, etc.  To learn more about vLLM, please refer to the [paper](https://arxiv.org/abs/2309.06180) and [documentation](https://docs.vllm.ai/)."
msgstr "我们建议您在部署 Qwen 时尝试使用 [vLLM](https://github.com/vllm-project/vllm)。它易于使用，且具有最先进的服务吞吐量、高效的注意力键值内存管理（通过PagedAttention实现）、连续批处理输入请求、优化的CUDA内核等功能。要了解更多关于vLLM的信息，请参阅 [论文](https://arxiv.org/abs/2309.06180) 和 [文档](https://docs.vllm.ai/)。"

#: ../../Qwen/source/deployment/vllm.md:7 b6e6f5a91b9e4b749a6aaeca89358752
msgid "Environment Setup"
msgstr "环境配置"

#: ../../Qwen/source/deployment/vllm.md:9 14f9a1b8015d45388f5681145ddfcb0b
msgid "By default, you can install `vllm` with pip in a clean environment:"
msgstr "默认情况下，你可以通过 pip 在新环境中安装 `vllm` ： "

#: ../../Qwen/source/deployment/vllm.md:15 fdda232bfb3c4d3895643b3ba7f78cbd
msgid "Please note that the prebuilt `vllm` has strict dependencies on `torch` and its CUDA versions. Check the note in the official document for installation ([link](https://docs.vllm.ai/en/latest/getting_started/installation.html)) for more help."
msgstr "请留意预构建的`vllm`对`torch`和其CUDA版本有强依赖。请查看[vLLM官方文档](https://docs.vllm.ai/en/latest/getting_started/installation.html)中的注意事项以获取有关安装的帮助。"

#: ../../Qwen/source/deployment/vllm.md:18 a175a37698bf4cfcb0c7bb33509e3775
msgid "API Service"
msgstr "API 服务"

#: ../../Qwen/source/deployment/vllm.md:20 6221b5708f054b839b17d3c21d086657
msgid "It is easy to build an OpenAI-compatible API service with vLLM, which can be deployed as a server that implements OpenAI API protocol. By default, it starts the server at `http://localhost:8000`.  You can specify the address with `--host` and `--port` arguments.  Run the command as shown below:"
msgstr "借助vLLM，构建一个与OpenAI API兼容的API服务十分简便，该服务可以作为实现OpenAI API协议的服务器进行部署。默认情况下，它将在 `http://localhost:8000` 启动服务器。您可以通过 `--host` 和 `--port` 参数来自定义地址。请按照以下所示运行命令："

#: ../../Qwen/source/deployment/vllm.md:28 5b3fb351eff5402caf53fb28af098a14
msgid "By default, if the model does not point to a valid local directory, it will download the model files from the HuggingFace Hub. To download model from ModelScope, set the following before running the above command:"
msgstr "默认情况下，如果模型未指向有效的本地目录，它将从 HuggingFace Hub 下载模型文件。要从 ModelScope 下载模型，请在运行上述命令之前设置以下内容："

#: ../../Qwen/source/deployment/vllm.md:34 e968dc2b83f94d8db88730e49cc2b557
msgid "For distrbiuted inference with tensor parallelism, it is as simple as"
msgstr "对于使用张量并行的分布式推理，操作非常简单："

#: ../../Qwen/source/deployment/vllm.md:38 a017a4820c164f99b1b818eff1ece7e2
msgid "The above command will use tensor parallelism on 4 GPUs. You should change the number of GPUs according to your demand."
msgstr "上述命令将在 4 块 GPU 上使用张量并行。您应根据需求调整 GPU 的数量。"

#: ../../Qwen/source/deployment/vllm.md:41 cf79ffc98aeb4aaeaa87eb67a27bf931
msgid "Basic Usage"
msgstr "基本用法"

#: ../../Qwen/source/deployment/vllm.md:43 f422098f08af453fba9a04ffba7a65cf
msgid "Then, you can use the [create chat interface](https://platform.openai.com/docs/api-reference/chat/completions/create) to communicate with Qwen:"
msgstr "然后，您可以利用 [create chat interface](https://platform.openai.com/docs/api-reference/chat/completions/create) 来与Qwen进行对话："

#: ../../Qwen/source/deployment/vllm.md 4fe8fdefc345451692648e733e009f2f
#: a8b7164993794f1398b4fb97662752d5
msgid "curl"
msgstr ""

#: ../../Qwen/source/deployment/vllm.md 5e2fab5952fd4fca952fb4d6bbca2a00
#: c5aff5ea28cd48bc8040b80173a609b8
msgid "Python"
msgstr ""

#: ../../Qwen/source/deployment/vllm.md:63
#: ../../Qwen/source/deployment/vllm.md:127 648478738dd3476c8dcdaa99cd345bfe
#: aa39dbf0acd646afa03c5fe79bb74011
msgid "You can use the API client with the `openai` Python SDK as shown below:"
msgstr "或者您可以如下面所示使用 `openai` Python SDK中的 API 客户端："

#: ../../Qwen/source/deployment/vllm.md:91 a4dc5343d3214279828ee2a8e8d06106
msgid "`vllm` will use the sampling parameters from the `generation_config.json` in the model files."
msgstr "`vllm` 将使用模型文件中 `generation_config.json` 的采样参数。"

#: ../../Qwen/source/deployment/vllm.md:93 ba2912bb69f64837887bc32f1107b9f0
msgid "While the default sampling parameters would work most of the time for thinking mode, it is recommended to adjust the sampling parameters according to your application,  and always pass the sampling parameters to the API."
msgstr "虽然默认的采样参数在大多数情况下适用于思考模式，但建议根据您的应用调整采样参数，并始终将采样参数传递给 API。"

#: ../../Qwen/source/deployment/vllm.md:99 9a47113d7a7b44b89f504244fada649f
msgid "Thinking & Non-Thinking Modes"
msgstr "思考与非思考模式"

#: ../../Qwen/source/deployment/vllm.md:101 20105d15da5a44bc8af9f9d628b54cb3
msgid "Qwen3 models will think before respond. This behaviour could be controled by either the hard switch, which could disable thinking completely, or the soft switch, where the model follows the instruction of the user on whether or not it should think."
msgstr "Qwen3 模型会在回复前进行思考。这种行为可以通过硬开关（完全禁用思考）或软开关（模型遵循用户关于是否应该思考的指令）来控制。"

#: ../../Qwen/source/deployment/vllm.md:104 df2e0e8d7b77407ba31e9efcea1440cf
msgid "The hard switch is availabe in vLLM through the following configuration to the API call. To disable thinking, use"
msgstr "硬开关在 vLLM 中可以通过以下 API 调用配置使用。要禁用思考，请使用"

#: ../../Qwen/source/deployment/vllm.md:156 31b40d3493e1487b8624048e64b07321
msgid "It is recommended to set sampling parameters differently for thinking and non-thinking modes."
msgstr "建议为思考模式和非思考模式分别设置不同的采样参数。"

#: ../../Qwen/source/deployment/vllm.md:159 ca30c135f89d4d0f9297484617c2c291
msgid "Parsing Thinking Content"
msgstr "解析思考内容"

#: ../../Qwen/source/deployment/vllm.md:161 8f8766d09dcc42dab5dfeba44e9495f0
msgid "vLLM supports parsing the thinking content from the model generation into structured messages:"
msgstr "vLLM 支持将模型生成的思考内容解析为结构化消息："

#: ../../Qwen/source/deployment/vllm.md:166 ce2e2c1c45804d758aaa536b2a134236
msgid "The response message will have a field named `reasoning_content` in addition to `content`, containing the thinking content generated by the model."
msgstr "响应消息除了包含 `content` 字段外，还会有一个名为 `reasoning_content` 的字段，其中包含模型生成的思考内容。"

#: ../../Qwen/source/deployment/vllm.md:169 5740a6011e2e4b7194c8b3ede0ede490
msgid "Please note that this feature is not OpenAI API compatible."
msgstr "请注意，此功能与 OpenAI API 规范不一致。"

#: ../../Qwen/source/deployment/vllm.md:172 4bab7e0f542e42c1a6834649c677a13f
msgid "Parsing Tool Calls"
msgstr "解析工具调用"

#: ../../Qwen/source/deployment/vllm.md:174 0285408a203d41ff9b7e35f216f911f3
msgid "vLLM supports parsing the tool calling content from the model generation into structured messages:"
msgstr "vLLM 支持将模型生成的工具调用内容解析为结构化消息："

#: ../../Qwen/source/deployment/vllm.md:179 b73b1c7b953c4cf9ab0fc4c7f7cce27f
msgid "For more information, please refer to [our guide on Function Calling](../framework/function_call.md#vllm)."
msgstr "详细信息，请参阅[函数调用的指南](../framework/function_call.md#vllm)。"

#: ../../Qwen/source/deployment/vllm.md:182 768aec6e39424dd1835c56497f3f9c19
msgid "As of vLLM 0.5.4, it is not supported to parse the thinking content and the tool calling from the model generation at the same time."
msgstr "在 vLLM 0.5.4 版本中，尚不支持同时解析模型生成的思考内容和工具调用。"

#: ../../Qwen/source/deployment/vllm.md:185 51dc25116a9346849b9c25436c64e770
msgid "Structured/JSON Output"
msgstr "结构化/JSON输出"

#: ../../Qwen/source/deployment/vllm.md:187 abfa9f6a9eb942d5878241295a2fd7d2
msgid "vLLM supports structured/JSON output.  Please refer to [vLLM's documentation](https://docs.vllm.ai/en/stable/serving/openai_compatible_server.html#extra-parameters-for-chat-api) for the `guided_json` parameters. Besides, it is also recommended to instruct the model to generate the specific format in the system message or in your prompt."
msgstr "vLLM 支持结构化/JSON 输出。请参照[vLLM文档](https://docs.vllm.ai/en/stable/serving/openai_compatible_server.html#extra-parameters-for-chat-api)了解 `guided_json` 参数。此外，也建议在系统消息或用户提示中指示模型生成特定格式，避免仅依赖于推理参数配置。"

#: ../../Qwen/source/deployment/vllm.md:192 d84c568411164a8c8259228be8f6433a
msgid "Serving Quantized models"
msgstr "部署量化模型"

#: ../../Qwen/source/deployment/vllm.md:194 d3b9d138d1ec4bf88eec3130a92c8d32
msgid "Qwen3 comes with two types of pre-quantized models, FP8 and AWQ."
msgstr "Qwen3 提供了两种类型的预量化模型：FP8 和 AWQ。"

#: ../../Qwen/source/deployment/vllm.md:196 c16aa9098f9e4faf8b19df4430181d1c
msgid "The command serving those models are the same as the original models except for the name change:"
msgstr "部署这些模型的命令与原始模型相同，只是名称有所更改："

#: ../../Qwen/source/deployment/vllm.md:206 774a3f0a10914f988aff2609b02ccb4c
msgid "FP8 computation is supported on NVIDIA GPUs with compute capability > 8.9, that is, Ada Lovelace, Hopper, and later GPUs."
msgstr "FP8 计算在计算能力 > 8.9 的 NVIDIA GPU 上受支持，即 Ada Lovelace、Hopper 及更新的 GPU。"

#: ../../Qwen/source/deployment/vllm.md:208 71a5529b10014da598a308aed2ff81cb
msgid "FP8 models will run on compute capability > 8.0 (Ampere) as weight-only W8A16, utilizing FP8 Marlin."
msgstr ""msgstr "FP8 模型将在计算能力 > 8.0（Ampere）的 GPU 上以仅权重 W8A16 的形式运行，利用 FP8 Marlin 技术。"

#: ../../Qwen/source/deployment/vllm.md:212 66310479fe11424d926533edd6d21dd0
msgid "As of vLLM 0.5.4, there are currently compatibility issues with `vllm` with the Qwen3 FP8 checkpoints.  For a quick fix, you should make the following changes to the file `vllm/vllm/model_executor/layers/linear.py`:"
msgstr "在 vLLM 0.5.4 版本中，目前 `vllm` 与 Qwen3 FP8 检查点存在兼容性问题。要快速解决此问题，您应对文件 `vllm/vllm/model_executor/layers/linear.py` 进行以下更改："

#: ../../Qwen/source/deployment/vllm.md:236 e10bf662530d4be884f61474a197df6f
msgid "Context Length"
msgstr "上下文长度"

#: ../../Qwen/source/deployment/vllm.md:238 100e104b60fb418b8c79ed341092efaa
msgid "The context length for Qwen3 models in pretraining is up to 32,768 tokenns. To handle context length substantially exceeding 32,768 tokens, RoPE scaling techniques should be applied. We have validated the performance of [YaRN](https://arxiv.org/abs/2309.00071), a technique for enhancing model length extrapolation, ensuring optimal performance on lengthy texts."
msgstr "Qwen3 模型在预训练中的上下文长度最长为 32,768 个 token。为了处理显著超过 32,768 个 token 的上下文长度，应应用 RoPE 缩放技术。我们已经验证了 [YaRN](https://arxiv.org/abs/2309.00071) 的性能，这是一种增强模型长度外推的技术，可确保在长文本上的最佳性能。"

#: ../../Qwen/source/deployment/vllm.md:242 3f987cfdb9114eb7be76a18cd0d01a1a
msgid "vLLM supports YaRN, which can be configured as"
msgstr "vLLM 支持 YaRN，可以配置为"

#: ../../Qwen/source/deployment/vllm.md:248 fb9ef4ed4c9640a18574830368379d15
msgid "vLLM implements static YaRN, which means the scaling factor remains constant regardless of input length, **potentially impacting performance on shorter texts.** We advise adding the `rope_scaling` configuration only when processing long contexts is required.  It is also recommended to modify the `factor` as needed. For example, if the typical context length for your application is 65,536 tokens, it would be better to set `factor` as 2.0."
msgstr "vLLM 实现了静态 YaRN，这意味着无论输入长度如何，缩放因子都保持不变，**这可能会对较短文本的性能产生影响。** 我们建议仅在需要处理长上下文时添加 `rope_scaling` 配置。还建议根据需要调整 `factor`。例如，如果您的应用程序的典型上下文长度为 65,536 个 token，则最好将 `factor` 设置为 2.0。"

#: ../../Qwen/source/deployment/vllm.md:254 e670bcf6664e490c8b5a7e0cc4ebba41
msgid "The default `max_position_embeddings` in `config.json` is set to 40,960, which used by vLLM, if `--max-model-len` is not specified. This allocation includes reserving 32,768 tokens for outputs and 8,192 tokens for typical prompts, which is sufficient for most scenarios involving short text processing and leave adequate room for model thinking. If the average context length does not exceed 32,768 tokens, we do not recommend enabling YaRN in this scenario, as it may potentially degrade model performance."
msgstr "如果未指定 `--max-model-len`，`config.json` 中的默认 `max_position_embeddings` 被设置为 40,960，vLLM 将使用该值。此分配包括为输出保留 32,768 个 token，为典型提示保留 8,192 个 token，这足以应对大多数涉及短文本处理的场景，并为模型思考留出充足空间。如果平均上下文长度不超过 32,768 个 token，我们不建议在此场景中启用 YaRN，因为这可能会降低模型性能。"

#: ../../Qwen/source/deployment/vllm.md:259 386a1e853cbb4ec38ea7050f21bdd0d8
msgid "Python Library"
msgstr "Python 库使用"

#: ../../Qwen/source/deployment/vllm.md:261 8ec4de6067e24624807613c89745e894
msgid "vLLM can also be directly used as a Python library, which is convinient for offline batch inference but lack some API-only features, such as parsing model generation to structure messages."
msgstr "vLLM 也可以直接用作 Python 库，这对离线批量推理非常方便，但缺少一些仅限 API 的功能，例如将模型生成解析为结构化消息。"

#: ../../Qwen/source/deployment/vllm.md:263 b3722c7e75bd44e5a6b2c3b7f44fd30f
msgid "The following shows the basic usage of vLLM as a library:"
msgstr "以下展示了将 vLLM 用作库的基本用法："

#: ../../Qwen/source/deployment/vllm.md:300 534845226be74248bd11a9b93fa153a0
msgid "FAQ"
msgstr "常见问题解答"

#: ../../Qwen/source/deployment/vllm.md:302 d1d72b72ccdc459c85ad8e383207931c
msgid "You may encounter OOM issues that are pretty annoying. We recommend two arguments for you to make some fix."
msgstr "您可能会遇到令人烦恼的OOM（内存溢出）问题。我们推荐您尝试两个参数进行修复。"

#: ../../Qwen/source/deployment/vllm.md:305 e5c77a3ba017433cb6b1a5c7e6015863
msgid "The first one is `--max-model-len`. Our provided default `max_position_embedding` is `40960` and thus the maximum length for the serving is also this value, leading to higher requirements of memory. Reducing it to a proper length for yourself often helps with the OOM issue."
msgstr "第一个参数是 `--max-model-len` 。我们提供的默认最大位置嵌入（`max_position_embedding`）为 40960 ，因此服务时的最大长度也是这个值，这会导致更高的内存需求。将此值适当减小通常有助于解决OOM问题。"

#: ../../Qwen/source/deployment/vllm.md:308 dc513e2855454776a4902cc8381b6c72
msgid "Another argument you can pay attention to is `--gpu-memory-utilization`. vLLM will pre-allocate this much GPU memory. By default, it is `0.9`. This is also why you find a vLLM service always takes so much memory. If you are in eager mode (by default it is not), you can level it up to tackle the OOM problem. Otherwise, CUDA Graphs are used, which will use GPU memory not controlled by vLLM, and you should try lowering it. If it doesn't work, you should try `--enforce-eager`, which may slow down infernece, or reduce the `--max-model-len`."
msgstr "另一个您可以关注的参数是 `--gpu-memory-utilization` 。 vLLM将预分配该参数指定比例的显存。默认情况下，该值为 `0.9`。这也是为什么您发现一个vLLM服务总是占用大量内存的原因。如果你使用了eager模式（默认不是），您可以将其调高以应对OOM问题。反之，vLLM会使用CUDA Graphs，而CUDA Graphs会额外占用不受vLLM管理的显存；此时，您应当尝试降低`--gpu-memory-utilization`。如果还是无法解决，可以尝试`--enforce-eager`（这会影响推理效率）或缩小`--max-model-len`。"

#~ msgid "Installation"
#~ msgstr "安装"

#~ msgid "Offline Batched Inference"
#~ msgstr "离线推理"

#~ msgid "Models supported by Qwen2.5 codes are supported by vLLM. The simplest usage of vLLM is offline batched inference as demonstrated below."
#~ msgstr "Qwen2.5代码支持的模型都被vLLM所支持。 vLLM最简单的使用方式是通过以下演示进行离线批量推理。"

#~ msgid "OpenAI-Compatible API Service"
#~ msgstr "OpenAI兼容的API服务"

#~ msgid "You don't need to worry about chat template as it by default uses the chat template provided by the tokenizer."
#~ msgstr "你无需担心chat模板，因为它默认会使用由tokenizer提供的chat模板。"

#~ msgid "The OpenAI-compatible server in `vllm` comes with [a default set of sampling parameters](https://github.com/vllm-project/vllm/blob/v0.5.2/vllm/entrypoints/openai/protocol.py#L130), which are not suitable for Qwen2.5 models and prone to repetition. We advise you to always pass sampling parameters to the API."
#~ msgstr "`vllm` 中的 OpenAI 兼容服务器使用 [一组默认的采样参数](https://github.com/vllm-project/vllm/blob/v0.5.2/vllm/entrypoints/openai/protocol.py#L130)。这组默认参数并不适用于 Qwen2.5 模型，并可能加重重复问题。我们建议您总是为该API传入合适的采样参数。"

#~ msgid "Tool Use"
#~ msgstr "工具使用"

#~ msgid "Multi-GPU Distributed Serving"
#~ msgstr "多卡分布式部署"

#~ msgid "To scale up your serving throughput, distributed serving helps you by leveraging more GPU devices.  Besides, for large models like `Qwen2.5-72B-Instruct`, it is impossible to serve it on a single GPU. Here, we demonstrate how to run `Qwen2.5-72B-Instruct` with tensor parallelism just by passing in the argument `tensor_parallel_size`:"
#~ msgstr "要提高模型的处理吞吐量，分布式服务可以通过利用更多的GPU设备来帮助您。特别是对于像 `Qwen2.5-72B-Instruct` 这样的大模型，单个GPU无法支撑其在线服务。在这里，我们通过演示如何仅通过传入参数 `tensor_parallel_size` ，来使用张量并行来运行 `Qwen2.5-72B-Instruct` 模型："

#~ msgid "Offline"
#~ msgstr "离线推理"

#~ msgid "API"
#~ msgstr ""

#~ msgid "Extended Context Support"
#~ msgstr "上下文支持扩展"

#~ msgid "vLLM supports YARN and it can be enabled by add a `rope_scaling` field to the `config.json` file of the model. For example,"
#~ msgstr "vLLM 支持 YaRN，并且可以通过在模型的 `config.json` 文件中添加一个 `rope_scaling` 字段来启用它。例如，"

#~ msgid "vLLM supports different types of quantized models, including AWQ, GPTQ, SqueezeLLM, etc.  Here we show how to deploy AWQ and GPTQ models.  The usage is almost the same as above except for an additional argument for quantization.  For example, to run an AWQ model. e.g., `Qwen2.5-7B-Instruct-AWQ`:"
#~ msgstr "vLLM 支持多种类型的量化模型，例如 AWQ、GPTQ、SqueezeLLM 等。这里我们将展示如何部署 AWQ 和 GPTQ 模型。使用方法与上述基本相同，只不过需要额外指定一个量化参数。例如，要运行一个 AWQ 模型，例如 `Qwen2.5-7B-Instruct-AWQ` ："

#~ msgid "or GPTQ models like `Qwen2.5-7B-Instruct-GPTQ-Int4`:"
#~ msgstr "或者是GPTQ模型比如 `Qwen2.5-7B-Instruct-GPTQ-Int4` ："

#~ msgid "Additionally, vLLM supports the combination of AWQ or GPTQ models with KV cache quantization, namely FP8 E5M2 KV Cache.  For example,"
#~ msgstr "此外，vLLM支持将AWQ或GPTQ模型与KV缓存量化相结合，即FP8 E5M2 KV Cache方案。例如："

#~ msgid "Troubleshooting"
#~ msgstr "常见问题"

