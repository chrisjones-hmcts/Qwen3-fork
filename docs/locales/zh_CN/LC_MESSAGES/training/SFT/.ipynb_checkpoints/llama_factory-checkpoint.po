# Copyright (C) 2024, Qwen Team, Alibaba Group.
# This file is distributed under the same license as the Qwen package.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: Qwen \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2024-03-12 01:42+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.14.0\n"

#: ../../source/training/SFT/llama_factory.rst:2
#: 59f81b6b7fa440c9b569151bd34595fd
msgid "LLaMA-Factory"
msgstr ""

#: ../../source/training/SFT/llama_factory.rst:4
#: 1d5a0e9c836a43e984330f7b50e84487
msgid ""
"Here we provide a script for supervised finetuning Qwen1.5 with `LLaMA-"
"Factory <https://github.com/hiyouga/LLaMA-Factory>`__. This script for "
"supervised finetuning (SFT) has the following features:"
msgstr "我们将介绍如何使用 `LLaMA-Factory <https://github.com/hiyouga/LLaMA-Factory>`__ 微调模型。本脚本包含如下特点："

#: ../../source/training/SFT/llama_factory.rst:8
#: ebe5c05bb39346f0b64ef71ec2f2c0a7
msgid "Support single-GPU and multi-GPU training;"
msgstr "支持单卡和多卡分布式训练"

#: ../../source/training/SFT/llama_factory.rst:10
#: 28e8b2797a93491591eff4fcb6f1a4ea
msgid "Support full-parameter tuning, LoRA, Q-LoRA, Dora."
msgstr "支持全参数微调、LoRA、Q-LoRA 和 DoRA 。"

#: ../../source/training/SFT/llama_factory.rst:12
#: 5705aea24e634555a9c1ac297415c062
msgid "In the following, we introduce more details about the usage of the script."
msgstr "下文将介绍更多关于脚本的用法。"

#: ../../source/training/SFT/llama_factory.rst:16
#: a946af86f08044379906c17ecafed11a
msgid "Installation"
msgstr "安装"

#: ../../source/training/SFT/llama_factory.rst:18
#: 2c970d53e2e04cdc910f62a0b77ed626
msgid "Before you start, make sure you have installed the following packages:"
msgstr "开始之前，确保你已经安装了以下代码库："

#: ../../source/training/SFT/llama_factory.rst:20
#: 0e3e1ec5879340e98f4e25e4b6d9ded1
msgid ""
"Follow the instructions of `LLaMA-Factory <https://github.com/hiyouga"
"/LLaMA-Factory>`__, and build the environment."
msgstr ""
"根据 `LLaMA-Factory <https://github.com/hiyouga/LLaMA-Factory>`__ "
"官方指引构建好你的环境"

#: ../../source/training/SFT/llama_factory.rst:23
#: 166ac903c6ce4285841641f5f3b5bec0
msgid "Install these packages (Optional):"
msgstr "安装下列代码库（可选）："

#: ../../source/training/SFT/llama_factory.rst:30
#: 5092a8b55dba46ca95ac6e177af0fa99
msgid ""
"If you want to use `FlashAttention-2 <https://github.com/Dao-AILab/flash-"
"attention>`__, make sure your CUDA is 11.6 and above."
msgstr ""
"如你使用 `FlashAttention-2 <https://github.com/Dao-AILab/flash-attention>`__"
"  ，请确保你的CUDA版本在11.6以上。"

#: ../../source/training/SFT/llama_factory.rst:35
#: ee8e36c067c94af2a4a74238f3d26ac5
msgid "Data Preparation"
msgstr "准备数据"

#: ../../source/training/SFT/llama_factory.rst:37
#: bad133d73f4049459f22f7c2cbf0523e
msgid ""
"LLaMA-Factory provides several training datasets in ``data`` folder, you "
"can use it directly. If you are using a custom dataset, please prepare "
"your dataset as follow."
msgstr ""
"LLaMA-Factory 在 ``data`` "
"文件夹中提供了多个训练数据集，您可以直接使用它们。如果您打算使用自定义数据集，请按照以下方式准备您的数据集。"

#: ../../source/training/SFT/llama_factory.rst:41
#: 6d942c4471c448b9a67a327d67442b34
msgid ""
"Organize your data in a **json** file and put your data in ``data`` "
"folder. LLaMA-Factory supports dataset in ``alpaca`` or ``sharegpt`` "
"format."
msgstr ""
"请将您的数据以 ``json`` 格式进行组织，并将数据放入 data 文件夹中。LLaMA-Factory 支持以 ``alpaca`` 或 "
"``sharegpt`` 格式的数据集。"

#: ../../source/training/SFT/llama_factory.rst:45
#: c80fd006f8bf481f806582eb000215e8
msgid "The dataset in ``alpaca`` format should follow the below format:"
msgstr "``alpaca`` 格式的数据集应遵循以下格式："

#: ../../source/training/SFT/llama_factory.rst:62
#: 876409cf2a294398926fdcbe355fd4d9
msgid "The dataset in ``sharegpt`` format should follow the below format:"
msgstr "``sharegpt`` 格式的数据集应遵循以下格式："

#: ../../source/training/SFT/llama_factory.rst:83
#: 3731a5252e4749efb1f5b897934094a7
msgid ""
"Provide your dataset definition in ``data/dataset_info.json`` in the "
"following format ."
msgstr "在 ``data/dataset_info.json`` 文件中提供您的数据集定义，并采用以下格式："

#: ../../source/training/SFT/llama_factory.rst:86
#: c048531148ff496dba3e8f57feb11e34
msgid ""
"For ``alpaca`` format dataset, the columns in ``dataset_info.json`` "
"should be:"
msgstr "对于 ``alpaca`` 格式的数据集，其 ``dataset_info.json`` 文件中的列应为："

#: ../../source/training/SFT/llama_factory.rst:102
#: 6ece984e56c4463a80b71f8af62282fe
msgid ""
"For ``sharegpt`` format dataset, the columns in ``dataset_info.json`` "
"should be:"
msgstr "对于 ``sharegpt`` 格式的数据集，``dataset_info.json`` 文件中的列应该包括："

#: ../../source/training/SFT/llama_factory.rst:124
#: 6da6d3f24fcf46ef86a08820e79446dd
msgid "Training"
msgstr "训练"

#: ../../source/training/SFT/llama_factory.rst:126
#: f3a81032d0924cb9b7dacdad9b32b4fc
msgid "Execute the following training command:"
msgstr "执行下列命令："

#: ../../source/training/SFT/llama_factory.rst:166
#: fc83cc671a04458db38f7ac6d3fee22b
msgid ""
"and enjoy the training process. To make changes to your training, you can"
" modify the arguments in the training command to adjust the "
"hyperparameters. One argument to note is ``cutoff_len``, which is the "
"maximum length of the training data. Control this parameter to avoid OOM "
"error."
msgstr ""
"并享受训练过程。若要调整您的训练，您可以通过修改训练命令中的参数来调整超参数。其中一个需要注意的参数是 ``cutoff_len`` "
"，它代表训练数据的最大长度。通过控制这个参数，可以避免出现OOM（内存溢出）错误。"

#: ../../source/training/SFT/llama_factory.rst:173
#: 663c9bd983754ac0aa20bd1dbb524598
msgid "Merge LoRA"
msgstr "合并LoRA"

#: ../../source/training/SFT/llama_factory.rst:175
#: 845a8a06b9494e589ee21b5c7ebaf22d
msgid ""
"If you train your model with LoRA, you probably need to merge adapter "
"parameters to the main branch. Run the following command to perform the "
"merging of LoRA adapters."
msgstr "如果你使用 LoRA 训练模型，可能需要将adapter参数合并到主分支中。请运行以下命令以执行 LoRA adapter 的合并操作。"

#: ../../source/training/SFT/llama_factory.rst:191
#: 03fd595a678f48cabe5b9c6192edc406
msgid "Conclusion"
msgstr "结语"

#: ../../source/training/SFT/llama_factory.rst:193
#: 3e566cad14514beaad47a28f282ada51
msgid ""
"The above content is the simplest way to use LLaMA-Factory to train Qwen."
" Feel free to dive into the details by checking the official repo!"
msgstr "上述内容是使用LLaMA-Factory训练Qwen的最简单方法。 欢迎通过查看官方仓库深入了解详细信息！"

