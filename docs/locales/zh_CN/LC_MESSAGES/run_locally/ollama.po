# Copyright (C) 2024, Qwen Team, Alibaba Group.
# This file is distributed under the same license as the Qwen package.
#
msgid ""
msgstr ""
"Project-Id-Version: Qwen \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-04-28 19:42+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: zh_CN\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.17.0\n"

#: ../../Qwen/source/run_locally/ollama.md:1 b115922199224f47bb532b26bf224620
msgid "Ollama"
msgstr "Ollama"

#: ../../Qwen/source/run_locally/ollama.md:4 d0b6051e7f0744dc894cdaa685a03e14
msgid "To be updated for Qwen3."
msgstr "仍需为Qwen3更新。"

#: ../../Qwen/source/run_locally/ollama.md:7 8e9346ee9e40484ca07e522a28c10a0f
msgid "[Ollama](https://ollama.com/) helps you run LLMs locally with only a few commands. It is available at MacOS, Linux, and Windows. Now, Qwen2.5 is officially on Ollama, and you can run it with one command:"
msgstr "[Ollama](https://ollama.com/)帮助您通过少量命令即可在本地运行LLM。它适用于MacOS、Linux和Windows操作系统。现在，Qwen2.5正式上线Ollama，您只需一条命令即可运行它："

#: ../../Qwen/source/run_locally/ollama.md:15 d320e9f92d7c4e91a6dc80e999b28823
msgid "Next, we introduce more detailed usages of Ollama for running Qwen2.5 models."
msgstr "接着，我们介绍在Ollama使用Qwen2.5模型的更多用法"

#: ../../Qwen/source/run_locally/ollama.md:17 51b225372f6c4b94af27930e3aaefd7e
msgid "Quickstart"
msgstr "快速开始"

#: ../../Qwen/source/run_locally/ollama.md:19 12459a52b35b401085439f7bb53f911d
msgid "Visit the official website [Ollama](https://ollama.com/) and click download to install Ollama on your device. You can also search models on the website, where you can find the Qwen2.5 models. Except for the default one, you can choose to run Qwen2.5-Instruct models of different sizes by:"
msgstr "访问官方网站[Ollama](https://ollama.com/)，点击`Download`以在您的设备上安装Ollama。您还可以在网站上搜索模型，在这里您可以找到Qwen2.5系列模型。除了默认模型之外，您可以通过以下方式选择运行不同大小的Qwen2.5-Instruct模型："

#: ../../Qwen/source/run_locally/ollama.md:23 0367670c6d70484b9be2532d1c29e185
msgid "`ollama run qwen2.5:0.5b`"
msgstr ""

#: ../../Qwen/source/run_locally/ollama.md:24 ab112db6e61547109ce8ea0c4c012471
msgid "`ollama run qwen2.5:1.5b`"
msgstr ""

#: ../../Qwen/source/run_locally/ollama.md:25 183f671bddee433faf2a4a329dcb55eb
msgid "`ollama run qwen2.5:3b`"
msgstr ""

#: ../../Qwen/source/run_locally/ollama.md:26 36a8777ecbeb48769efec7ece108735d
msgid "`ollama run qwen2.5:7b`"
msgstr ""

#: ../../Qwen/source/run_locally/ollama.md:27 f4651bc149b94406b85096f4783c5e53
msgid "`ollama run qwen2.5:14b`"
msgstr ""

#: ../../Qwen/source/run_locally/ollama.md:28 6c9850d60512459988a6b73a6b1e01fc
msgid "`ollama run qwen2.5:32b`"
msgstr ""

#: ../../Qwen/source/run_locally/ollama.md:29 a96e5ef6669c485399d85ef441f6306c
msgid "`ollama run qwen2.5:72b`"
msgstr ""

#: ../../Qwen/source/run_locally/ollama.md:32 3e61ed7de75245559bbca559ded82630
msgid "`ollama` does not host base models. Even though the tag may not have the instruct suffix, they are all instruct models."
msgstr "`ollama`并不托管基模型。即便模型标签不带instruct后缀，实际也是instruct模型。"

#: ../../Qwen/source/run_locally/ollama.md:36 85b7620bbb1e4d38833b5a231e9f12c2
msgid "Run Ollama with Your GGUF Files"
msgstr "用Ollama运行你自己的GGUF文件"

#: ../../Qwen/source/run_locally/ollama.md:38 29df4213bffb4bc7a970d7ce86ad275f
msgid "Sometimes you don't want to pull models and you just want to use Ollama with your own GGUF files. Suppose you have a GGUF file of Qwen2.5, `qwen2.5-7b-instruct-q5_0.gguf`. For the first step, you need to create a file called `Modelfile`. The content of the file is shown below:"
msgstr "有时您可能不想拉取模型，而是希望直接使用自己的GGUF文件来配合Ollama。假设您有一个名为`qwen2.5-7b-instruct-q5_0.gguf`的Qwen2.5的GGUF文件。在第一步中，您需要创建一个名为`Modelfile`的文件。该文件的内容如下所示："

#: ../../Qwen/source/run_locally/ollama.md:101 19c9382f6bfc4aeda355f5745988fadb
msgid "Then create the ollama model by running:"
msgstr "然后通过运行下列命令来创建一个ollama模型"

#: ../../Qwen/source/run_locally/ollama.md:107 4de4ae26fa644c83848aa97598160a02
msgid "Once it is finished, you can run your ollama model by:"
msgstr "完成后，你即可运行你的ollama模型："

#: ../../Qwen/source/run_locally/ollama.md:113 ce86e8ed78164b369eec2548b6260b49
msgid "Tool Use"
msgstr "工具调用"

#: ../../Qwen/source/run_locally/ollama.md:115 09644168e091461abc60e06d579f80b0
msgid "Tool use is now support Ollama and you should be able to run Qwen2.5 models with it. For more details, see our [function calling guide](../framework/function_call)."
msgstr "Ollama现已支持工具调用，Qwen2.5也已适配。更多详情，请参阅我们的[函数调用指南](../framework/function_call)"

