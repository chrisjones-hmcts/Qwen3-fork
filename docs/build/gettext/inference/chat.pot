# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2024, Qwen Team
# This file is distributed under the same license as the Qwen package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: Qwen \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2024-03-18 18:18+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: ../../source/inference/chat.rst:2
#: 7745a45637b641af8a3221d538c46d0a
msgid "Using Transformers to Chat"
msgstr ""

#: ../../source/inference/chat.rst:4
#: e822724d588b4e4c8c3b9134156083d7
msgid "The most significant but also the simplest usage of Qwen1.5 is to chat with it using the ``transformers`` library. In this document, we show how to chat with ``Qwen1.5-7B-Chat``, in either streaming mode or not."
msgstr ""

#: ../../source/inference/chat.rst:9
#: b0ec598e25f345a8ba443b3511b1d585
msgid "Basic Usage"
msgstr ""

#: ../../source/inference/chat.rst:11
#: 21c1149d415a4bd6a882c1f14e6c4522
msgid "You can just write several lines of code with ``transformers`` to chat with Qwen1.5-Chat. Essentially, we build the tokenizer and the model with ``from_pretrained`` method, and we use ``generate`` method to perform chatting with the help of chat template provided by the tokenizer. Below is an example of how to chat with Qwen1.5-7B-Chat:"
msgstr ""

#: ../../source/inference/chat.rst:56
#: 8de10bd6e37b44b0ae1d1782baf55149
msgid "Note that the previous method in the original Qwen repo ``chat()`` is now replaced by ``generate()``. The ``apply_chat_template()`` function is used to convert the messages into a format that the model can understand. The ``add_generation_prompt`` argument is used to add a generation prompt, which refers to ``<|im_start|>assistant\\n`` to the input. Notably, we apply ChatML template for chat models following our previous practice. The ``max_new_tokens`` argument is used to set the maximum length of the response. The ``tokenizer.batch_decode()`` function is used to decode the response. In terms of the input, the above ``messages`` is an example to show how to format your dialog history and system prompt. By default, if you do not specify system prompt, we directly use ``You are a helpful assistant.``."
msgstr ""

#: ../../source/inference/chat.rst:70
#: cbde0fbb75cd4680b4fbb05e2887c60d
msgid "Streaming Mode"
msgstr ""

#: ../../source/inference/chat.rst:72
#: b1f6a7264de647389ed4445f5bc780a9
msgid "With the help of ``TextStreamer``, you can modify your chatting with Qwen to streaming mode. Below we show you an example of how to use it:"
msgstr ""

#: ../../source/inference/chat.rst:89
#: 419be9ed047b411cbef431a43a8583e7
msgid "Besides using ``TextStreamer``, we can also use ``TextIteratorStreamer`` which stores print-ready text in a queue, to be used by a downstream application as an iterator:"
msgstr ""

#: ../../source/inference/chat.rst:111
#: 4e6d9f5251554ebca1385b7851896cbd
msgid "Next Step"
msgstr ""

#: ../../source/inference/chat.rst:113
#: d0e09594fef544e98c22923f0f641831
msgid "Now you can chat with Qwen1.5 in either streaming mode or not. Continue to read the documentation and try to figure out more advanced usages of model inference!"
msgstr ""
