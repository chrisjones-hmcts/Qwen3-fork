# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2024, Qwen Team
# This file is distributed under the same license as the Qwen package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: Qwen \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2024-03-18 18:18+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: ../../source/web_ui/text_generation_webui.rst:2
#: 5f36b0800b0542f79077361ac313f28d
msgid "Text Generation Web UI"
msgstr ""

#: ../../source/web_ui/text_generation_webui.rst:4
#: e11f1d62325148a89dd924e94ea9289b
msgid "`Text Generation Web UI <https://github.com/oobabooga/text-generation-webui>`__ (TGW, or usually referred to “oobabooga”) is a popular web UI for text generation, similar to `AUTOMATIC1111/stable-diffusion-webui <https://github.com/AUTOMATIC1111/stable-diffusion-webui>`__. It has multiple interfaces, and supports multiple model backends, including `Transformers <https://github.com/huggingface/transformers>`__, `llama.cpp <https://github.com/ggerganov/llama.cpp>`__ (through `llama-cpp-python <https://github.com/abetlen/llama-cpp-python>`__), `ExLlamaV2 <https://github.com/turboderp/exllamav2>`__, `AutoGPTQ <https://github.com/PanQiWei/AutoGPTQ>`__, `AutoAWQ <https://github.com/casper-hansen/AutoAWQ>`__, `GPTQ-for-LLaMa <https://github.com/qwopqwop200/GPTQ-for-LLaMa>`__, `CTransformers <https://github.com/marella/ctransformers>`__, `QuIP# <https://github.com/Cornell-RelaxML/quip-sharp>`__. In this section, we introduce how to run Qwen locally with TGW."
msgstr ""

#: ../../source/web_ui/text_generation_webui.rst:23
#: 2ff3abdd54954d17b95731a81fcf112d
msgid "Quickstart"
msgstr ""

#: ../../source/web_ui/text_generation_webui.rst:25
#: e14612c9b79c41da87f2402e35c88cc6
msgid "The simplest way to run TGW is to use the provided shell scripts in the `repo <https://github.com/oobabooga/text-generation-webui>`__. For the first step, clone the repo and enter the directory:"
msgstr ""

#: ../../source/web_ui/text_generation_webui.rst:34
#: a24cf75ac66a4b54a168816d9cade7b5
msgid "You can directly run the ``start_linux.sh``, ``start_windows.bat``, ``start_macos.sh``, or ``start_wsl.bat`` script depending on your OS. Alternatively you can manually install the requirements in your conda environment. Here I take the practice on MacOS as an example."
msgstr ""

#: ../../source/web_ui/text_generation_webui.rst:45
#: 3470c6aaac8d4f2ab798cc1427a08034
msgid "Then you can install the requirements by running ``pip install -r`` based on your OS, e.g.,"
msgstr ""

#: ../../source/web_ui/text_generation_webui.rst:52
#: bd5584f9ad044423948a0d0f1376b331
msgid "For ``bitsandbytes`` and ``llama-cpp-python`` inside the requirements, I advise you to install them through ``pip`` directly. However, temporarily please do not use GGUF as the performance with TGW is unsatisfactory. After finishing the installation of required packages, you need to prepare your models by putting the model files or directories in the folder ``./models``. For example, you should put the transformers model directory of ``Qwen1.5-7B-Chat`` in the way shown below:"
msgstr ""

#: ../../source/web_ui/text_generation_webui.rst:76
#: 57cb5eb4e95f4968b6b9870e29bbf6db
msgid "Then you just need to run"
msgstr ""

#: ../../source/web_ui/text_generation_webui.rst:82
#: 7381af27556f40f282c97f987d6a18ea
msgid "to launch your web UI service. Please browse to"
msgstr ""

#: ../../source/web_ui/text_generation_webui.rst:88
#: f8329cc957764c5f95717bd91235ed24
msgid "and enjoy playing with Qwen in a web UI!"
msgstr ""

#: ../../source/web_ui/text_generation_webui.rst:91
#: 67ce930a2b194b6fa24f3eb179f57b23
msgid "Next Step"
msgstr ""

#: ../../source/web_ui/text_generation_webui.rst:93
#: 3092795e056c4154b429d3676be70891
msgid "There are a lot more usages in TGW, where you can even enjoy role play, use different types of quantized models, train LoRA, incorporate extensions like stable diffusion and whisper, etc. Go to figure out more advanced usages and apply them to Qwen models!"
msgstr ""
