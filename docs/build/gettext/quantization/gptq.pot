# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2024, Qwen Team
# This file is distributed under the same license as the Qwen package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: Qwen \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2024-03-18 18:18+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: ../../source/quantization/gptq.rst:2
#: a457597873874433bb1914600a9e0964
msgid "GPTQ"
msgstr ""

#: ../../source/quantization/gptq.rst:4
#: 5872f5f17e994d2b84d8f7d35e34e73f
msgid "`GPTQ <https://arxiv.org/abs/2210.17323>`__ is a quantization method for GPT-like LLMs, which uses one-shot weight quantization based on approximate second-order information. In this document, we show you how to use the quantized model with transformers and also how to quantize your own model with `AutoGPTQ <https://github.com/AutoGPTQ/AutoGPTQ>`__."
msgstr ""

#: ../../source/quantization/gptq.rst:11
#: e15b125ddca2457b8e8d0018d67f7a52
msgid "Usage of GPTQ Models with Transformers"
msgstr ""

#: ../../source/quantization/gptq.rst:13
#: 294c01ecf8d44c5aa960a96cc9520417
msgid "Now, Transformers has officially supported AutoGPTQ, which means that you can directly use the quantized model with Transformers. The following is a very simple code snippet showing how to run ``Qwen1.5-7B-Chat-GPTQ-Int8`` (note that for each size of Qwen1.5, we provide both Int4 and Int8 quantized models) with the quantized model:"
msgstr ""

#: ../../source/quantization/gptq.rst:53
#: b1a1bfb306954ef6b1f0ad415a5a39b6
msgid "Usage of GPTQ Quantized Models with vLLM"
msgstr ""

#: ../../source/quantization/gptq.rst:55
#: 1b965c52b4514a7aa4731c0b1b3e5793
msgid "vLLM has supported GPTQ, which means that you can directly use our provided GPTQ models or those trained with ``AutoGPTQ`` with vLLM. Actually, the usage is the same with the basic usage of vLLM. We provide a simple example of how to launch OpenAI-API compatible API with vLLM and ``Qwen1.5-7B-Chat-GPTQ-Int8``:"
msgstr ""

#: ../../source/quantization/gptq.rst:75
#: a717aebb3e40430d9e8c305b6e9a2596
msgid "or you can use python client with ``openai`` python package as shown below:"
msgstr ""

#: ../../source/quantization/gptq.rst:100
#: 6808f53cdf074c89a107281bfd738a43
msgid "Quantize Your Own Model with AutoGPTQ"
msgstr ""

#: ../../source/quantization/gptq.rst:102
#: e75d60870ae7457db17ff32386998748
msgid "If you want to quantize your own model to GPTQ quantized models, we advise you to use AutoGPTQ. It is suggested installing the latest version of the package by installing from source code:"
msgstr ""

#: ../../source/quantization/gptq.rst:112
#: 89fcabdb905d404a8d2ed786a886cd49
msgid "Suppose you have finetuned a model based on ``Qwen1.5-7B``, which is named ``Qwen1.5-7B-finetuned``, with your own dataset, e.g., Alpaca. To build your own GPTQ quantized model, you need to use the training data for calibration. Below, we provide a simple demonstration for you to run:"
msgstr ""

#: ../../source/quantization/gptq.rst:145
#: efd97a39b63a44f0974efe17c0e03eca
msgid "Then you need to prepare your data for calibaration. What you need to do is just put samples into a list, each of which is a text. As we directly use our finetuning data for calibration, we first format it with ChatML template. For example:"
msgstr ""

#: ../../source/quantization/gptq.rst:160
#: 12ae5542e47e4fcbbe4341430fd04435
msgid "where each ``msg`` is a typical chat message as shown below:"
msgstr ""

#: ../../source/quantization/gptq.rst:170
#: d4cdf9cebf4c48a08bb6b5deb5ebab98
msgid "Then just run the calibration process by one line of code:"
msgstr ""

#: ../../source/quantization/gptq.rst:179
#: d23c969069794371ac73c10e9b05d815
msgid "Finally, save the quantized model:"
msgstr ""

#: ../../source/quantization/gptq.rst:186
#: f32827a91c2642ef8697ce55d5e7e808
msgid "It is unfortunate that the ``save_quantized`` method does not support sharding. For sharding, you need to load the model and use ``save_pretrained`` from transformers to save and shard the model. Except for this, everything is so simple. Enjoy!"
msgstr ""
