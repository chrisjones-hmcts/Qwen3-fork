# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2024, Qwen Team
# This file is distributed under the same license as the Qwen package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: Qwen \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2024-03-18 18:18+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: ../../source/quantization/gguf.rst:2
#: 0501a99ef99947fb9f805ee34d169420
msgid "GGUF"
msgstr ""

#: ../../source/quantization/gguf.rst:4
#: 4389ee1a65f549ad88fdb4a5cbd0cfc3
msgid "Recently, running LLMs locally is popular in the community, and running GGUF files with llama.cpp is a typical example. With llama.cpp, you can not only build GGUF files for your models but also perform low-bit quantization. In GGUF, you can directly quantize your models without calibration, or apply the AWQ scale for better quality, or use imatrix with calibration data. In this document, we demonstrate the simplest way to quantize your model as well as the way to apply AWQ scale to your Qwen model quantization."
msgstr ""

#: ../../source/quantization/gguf.rst:14
#: 2e9373f9b3e54a1a9d01655a47c5204c
msgid "Quantize Your Models and Make GGUF Files"
msgstr ""

#: ../../source/quantization/gguf.rst:16
#: 126537c63ddb45878026658211681d38
msgid "Before you move to quantization, make sure you have followed the instruction and started to use llama.cpp. The following guidance will NOT provide instructions about installation and building. Now, suppose you would like to quantize ``Qwen1.5-7B-Chat``. You need to first make a GGUF file for the fp16 model as shown below:"
msgstr ""

#: ../../source/quantization/gguf.rst:26
#: a1372558e5744d1ca7291e610df70f88
msgid "where the first argument refers to the path to the HF model directory or the HF model name, and the second argument refers to the path of your output GGUF file (here I just put it under the directory ``models/7B``. Remember to create the directory before you run the command). In this way, you have generated a GGUF file for your fp16 model, and you then need to quantize it to low bits based on your requirements. An example of quantizing the model to 4 bits is shown below:"
msgstr ""

#: ../../source/quantization/gguf.rst:38
#: 780e75dde54045ffb920bb7d53ab53ce
msgid "where we use ``q4_0`` for the 4-bit quantization. Until now, you have finished quantizing a model to 4 bits and putting it into a GGUF file, which can be run directly with llama.cpp."
msgstr ""

#: ../../source/quantization/gguf.rst:43
#: d59569b733914315816a65135c70acdc
msgid "Quantize Your Models With AWQ Scales"
msgstr ""

#: ../../source/quantization/gguf.rst:45
#: ab57500798ae473b9c9e98c8dd69a773
msgid "To improve the quality of your quantized models, one possible solution is to apply the AWQ scale, following `this script <https://github.com/casper-hansen/AutoAWQ/blob/main/docs/examples.md>`__. First of all, when you run ``model.quantize()`` with AutoAWQ, remember to add ``export_compatible=True`` as shown below:"
msgstr ""

#: ../../source/quantization/gguf.rst:62
#: 2b0e3e5022e34027b7287fb617403e5b
msgid "With ``model.save_quantzed()`` as shown above, a fp16 model with AWQ scales is saved. Then, when you run ``convert-hf-to-gguf.py``, remember to replace the model path with the path to the fp16 model with AWQ scales, e.g.,"
msgstr ""

#: ../../source/quantization/gguf.rst:71
#: b4c96ac7ab8040e592278ebbd28b6deb
msgid "In this way, you can apply the AWQ scales to your quantized models in GGUF formats, which helps improving the model quality."
msgstr ""

#: ../../source/quantization/gguf.rst:74
#: 81cb97da72414da0b1b96066af1a041c
msgid "We usually quantize the fp16 model to 2, 3, 4, 5, 6, and 8-bit models. To perform different low-bit quantization, just replace the quantization method in your command. For example, if you want to quantize your model to 2-bit model, you can replace ``q4_0`` to ``q2_k`` as demonstrated below:"
msgstr ""

#: ../../source/quantization/gguf.rst:84
#: 7e90cca73c7b4e9f8bb2406b9e58f7a6
msgid "We now provide GGUF models in the following quantization levels: ``q2_k``, ``q3_k_m``, ``q4_0``, ``q4_k_m``, ``q5_0``, ``q5_k_m``, ``q6_k``, and ``q8_0``. For more information, please visit `llama.cpp <https://github.com/ggerganov/llama.cpp>`__."
msgstr ""
