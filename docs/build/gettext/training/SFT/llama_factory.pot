# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2024, Qwen Team
# This file is distributed under the same license as the Qwen package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: Qwen \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2024-03-18 18:18+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: ../../source/training/SFT/llama_factory.rst:2
#: 2c7cd522171244d1860362c40dd9f5b4
msgid "LLaMA-Factory"
msgstr ""

#: ../../source/training/SFT/llama_factory.rst:4
#: e57d91aa3e1d47198bead41af69f6028
msgid "Here we provide a script for supervised finetuning Qwen1.5 with `LLaMA-Factory <https://github.com/hiyouga/LLaMA-Factory>`__. This script for supervised finetuning (SFT) has the following features:"
msgstr ""

#: ../../source/training/SFT/llama_factory.rst:8
#: 5476881c3a83493c9ca5e8f875778151
msgid "Support single-GPU and multi-GPU training;"
msgstr ""

#: ../../source/training/SFT/llama_factory.rst:10
#: e45426ef5b1a4ba7b3b9a2849f105c47
msgid "Support full-parameter tuning, LoRA, Q-LoRA, Dora."
msgstr ""

#: ../../source/training/SFT/llama_factory.rst:12
#: 048773a021fe4d5b9492e8f2c8378357
msgid "In the following, we introduce more details about the usage of the script."
msgstr ""

#: ../../source/training/SFT/llama_factory.rst:16
#: 4fa9db3280674881b4dca9de1a0bae77
msgid "Installation"
msgstr ""

#: ../../source/training/SFT/llama_factory.rst:18
#: ed0945ac4d7c4ea7b24cc02c9e51cb04
msgid "Before you start, make sure you have installed the following packages:"
msgstr ""

#: ../../source/training/SFT/llama_factory.rst:20
#: 8e0c1ebab52e4b1db5bfc6aac5a6ec0d
msgid "Follow the instructions of `LLaMA-Factory <https://github.com/hiyouga/LLaMA-Factory>`__, and build the environment."
msgstr ""

#: ../../source/training/SFT/llama_factory.rst:23
#: 2121595688b849d99a665520194c6a4c
msgid "Install these packages (Optional):"
msgstr ""

#: ../../source/training/SFT/llama_factory.rst:30
#: 7a0d0beb23054c61ad9bddc4209b8a88
msgid "If you want to use `FlashAttention-2 <https://github.com/Dao-AILab/flash-attention>`__, make sure your CUDA is 11.6 and above."
msgstr ""

#: ../../source/training/SFT/llama_factory.rst:35
#: e2a8067f865a477aa1c99014451a0231
msgid "Data Preparation"
msgstr ""

#: ../../source/training/SFT/llama_factory.rst:37
#: 1beb02aa02df4873a37e4b92c1e993ff
msgid "LLaMA-Factory provides several training datasets in ``data`` folder, you can use it directly. If you are using a custom dataset, please prepare your dataset as follow."
msgstr ""

#: ../../source/training/SFT/llama_factory.rst:41
#: 7b8df6e7aa3d44d5b93352f6c62f9603
msgid "Organize your data in a **json** file and put your data in ``data`` folder. LLaMA-Factory supports dataset in ``alpaca`` or ``sharegpt`` format."
msgstr ""

#: ../../source/training/SFT/llama_factory.rst:45
#: b8cb53c4c3984a9a88449cd147f12f0d
msgid "The dataset in ``alpaca`` format should follow the below format:"
msgstr ""

#: ../../source/training/SFT/llama_factory.rst:62
#: 8a0c0a1d12474816a8bf3907f7206349
msgid "The dataset in ``sharegpt`` format should follow the below format:"
msgstr ""

#: ../../source/training/SFT/llama_factory.rst:83
#: 4fc05349c5ec4275a12f161ddfbf28eb
msgid "Provide your dataset definition in ``data/dataset_info.json`` in the following format ."
msgstr ""

#: ../../source/training/SFT/llama_factory.rst:86
#: 9d0442b3446d4223b7bc329c1fea783a
msgid "For ``alpaca`` format dataset, the columns in ``dataset_info.json`` should be:"
msgstr ""

#: ../../source/training/SFT/llama_factory.rst:102
#: 6a7d22d253534856992c22ff2b872e53
msgid "For ``sharegpt`` format dataset, the columns in ``dataset_info.json`` should be:"
msgstr ""

#: ../../source/training/SFT/llama_factory.rst:124
#: f00f267cd3e1409cb381d7c37c0467fd
msgid "Training"
msgstr ""

#: ../../source/training/SFT/llama_factory.rst:126
#: 34f52f4a416b49378f09ca2886dde979
msgid "Execute the following training command:"
msgstr ""

#: ../../source/training/SFT/llama_factory.rst:166
#: eb27aba1740741e09e71820a26c0cda1
msgid "and enjoy the training process. To make changes to your training, you can modify the arguments in the training command to adjust the hyperparameters. One argument to note is ``cutoff_len``, which is the maximum length of the training data. Control this parameter to avoid OOM error."
msgstr ""

#: ../../source/training/SFT/llama_factory.rst:173
#: 58b355399f8249e1b7119647e2f71486
msgid "Merge LoRA"
msgstr ""

#: ../../source/training/SFT/llama_factory.rst:175
#: d5883ed5f35240818c2d7d09315d05af
msgid "If you train your model with LoRA, you probably need to merge adapter parameters to the main branch. Run the following command to perform the merging of LoRA adapters."
msgstr ""

#: ../../source/training/SFT/llama_factory.rst:191
#: 1a5d7a3e5d6f45ed89260ebcb7b8c362
msgid "Conclusion"
msgstr ""

#: ../../source/training/SFT/llama_factory.rst:193
#: 16beef241c204ddd98d67d8cf18c02ac
msgid "The above content is the simplest way to use LLaMA-Factory to train Qwen. Feel free to dive into the details by checking the official repo!"
msgstr ""
