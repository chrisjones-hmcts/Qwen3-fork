# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2024, Qwen Team
# This file is distributed under the same license as the Qwen package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: Qwen \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2024-03-18 18:47+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: ../../source/framework/LlamaIndex.rst:2
#: fd8cb627291b4337a5343514cb875ce3
msgid "LlamaIndex"
msgstr ""

#: ../../source/framework/LlamaIndex.rst:4
#: 3814141ca63942118893f4f20a549b28
msgid "To connect Qwen1.5. with external data, such as documents, web pages, etc., we offer a tutorial on `LlamaIndex <https://www.llamaindex.ai/>`__. This guide helps you quickly implement retrieval-augmented generation (RAG) using LlamaIndex with Qwen1.5."
msgstr ""

#: ../../source/framework/LlamaIndex.rst:8
#: 9d50f10bb8b2432785823a536b89b902
msgid "Preparation"
msgstr ""

#: ../../source/framework/LlamaIndex.rst:10
#: fbb180e615474ca49345c4e6ace1bb74
msgid "To implement RAG, we advise you to install the LlamaIndex-related packages first."
msgstr ""

#: ../../source/framework/LlamaIndex.rst:13
#: 6b09fffd6967464fb87848e130421243
msgid "The following is a simple code snippet showing how to do this:"
msgstr ""

#: ../../source/framework/LlamaIndex.rst:22
#: 34c29b67ddcb4d5997c429bf7a6ecee8
msgid "Set Parameters"
msgstr ""

#: ../../source/framework/LlamaIndex.rst:24
#: 0e6d3c9deb384da3b328fae2199c0239
msgid "Now we can set up LLM, embedding model, and the related configurations. Qwen1.5-Chat supports conversations in multiple languages, including English and Chinese. You can use the ``bge-base-en-v1.5`` model to retrieve from English documents, and you can download the ``bge-base-zh-v1.5`` model to retrieve from Chinese documents. You can also choose ``bge-large`` or ``bge-small`` as the embedding model or modify the context window size or text chunk size depending on your computing resources. Qwen 1.5 model families support a maximum of 32K context window size."
msgstr ""

#: ../../source/framework/LlamaIndex.rst:82
#: 1243ca1a38c34d1797bae5518410374b
msgid "Build Index"
msgstr ""

#: ../../source/framework/LlamaIndex.rst:84
#: 5fba4b593336404199cb633cb37290ea
msgid "Now we can build index from documents or websites."
msgstr ""

#: ../../source/framework/LlamaIndex.rst:86
#: 6824493c7a0b4bc593374050a816b1bf
msgid "The following code snippet demonstrates how to build an index for files (regardless of whether they are in PDF or TXT format) in a local folder named 'document'."
msgstr ""

#: ../../source/framework/LlamaIndex.rst:99
#: ba2dd451ed214fce8b5dcd7d2605edad
msgid "The following code snippet demonstrates how to build an index for the content in a list of websites."
msgstr ""

#: ../../source/framework/LlamaIndex.rst:115
#: 0caf80e9b56344ba8551739c2c6e36d1
msgid "To save and load the index, you can use the following code snippet."
msgstr ""

#: ../../source/framework/LlamaIndex.rst:129
#: 9d6e83c2ffbd407cb88c383d88a396da
msgid "RAG"
msgstr ""

#: ../../source/framework/LlamaIndex.rst:131
#: 49c900e2323d4e3ca121fe25bdc30b38
msgid "Now you can perform queries, and Qwen1.5 will answer based on the content of the indexed documents."
msgstr ""
