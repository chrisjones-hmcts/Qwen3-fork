# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2024, Qwen Team
# This file is distributed under the same license as the Qwen package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: Qwen \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2024-03-18 18:18+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: ../../source/getting_started/quickstart.rst:2
#: 3ea63e7afd0f4ab582679f88cd9dfc8d
msgid "Quickstart"
msgstr ""

#: ../../source/getting_started/quickstart.rst:4
#: d9f1a429c21a47bf8e2d30ad8e6988c2
msgid "This guide helps you quickly start using Qwen1.5. We provide examples of `Hugging Face Transformers <https://github.com/huggingface/transformers>`__ as well as `ModelScope <https://github.com/modelscope/modelscope>`__, and `vLLM <https://github.com/vllm-project/vllm>`__ for deployment."
msgstr ""

#: ../../source/getting_started/quickstart.rst:10
#: 85139be037ab443c9c245ccec5a497a4
msgid "Hugging Face Transformers & ModelScope"
msgstr ""

#: ../../source/getting_started/quickstart.rst:12
#: ef7aee22525f406b8f12a4ee7bbd7602
msgid "To get a quick start with Qwen1.5, we advise you to try with the inference with ``transformers`` first. Make sure that you have installed ``transformers>=4.37.0``. The following is a very simple code snippet showing how to run Qwen1.5-Chat, with an example of Qwen1.5-7B-Chat:"
msgstr ""

#: ../../source/getting_started/quickstart.rst:56
#: 5049b15980d644a0a30afb201098a204
msgid "Previously, we use ``model.chat()`` (see ``modeling_qwen.py`` in previous Qwen models for more information). Now, we follow the practice of ``transformers`` and directly use ``model.generate()`` with ``apply_chat_template()`` in tokenizer."
msgstr ""

#: ../../source/getting_started/quickstart.rst:61
#: 5f8986e2557349c9afcd5a673ed21f20
msgid "To tackle with downloading issues, we advise you to try with from ModelScope, just changing the first line of code above to the following:"
msgstr ""

#: ../../source/getting_started/quickstart.rst:68
#: 4384560eaa524c419e07adb85005ff82
msgid "Streaming mode for model chat is simple with the help of ``TextStreamer``. Below we show you an example of how to use it:"
msgstr ""

#: ../../source/getting_started/quickstart.rst:84
#: b3d056593985417a888880da18a6f52d
msgid "vLLM for Deployment"
msgstr ""

#: ../../source/getting_started/quickstart.rst:86
#: f9c3aee6c08649d39c683106777b8bb4
msgid "To deploy Qwen1.5, we advise you to use vLLM. vLLM is a fast and easy-to-use framework for LLM inference and serving. In the following, we demonstrate how to build a OpenAI-API compatible API service with vLLM."
msgstr ""

#: ../../source/getting_started/quickstart.rst:91
#: 5fa24ef16a1242c887737295133c1f82
msgid "First, make sure you have installed ``vLLM>=0.3.0``:"
msgstr ""

#: ../../source/getting_started/quickstart.rst:97
#: 5da6c7860a5e4151ac97e1e268e15cbf
msgid "Run the following code to build up a vllm service. Here we take Qwen1.5-7B-Chat as an example:"
msgstr ""

#: ../../source/getting_started/quickstart.rst:104
#: b42ed3ac91f147eca604524772e6c1bb
msgid "Then, you can use the `create chat interface <https://platform.openai.com/docs/api-reference/chat/completions/create>`__ to communicate with Qwen:"
msgstr ""

#: ../../source/getting_started/quickstart.rst:118
#: 90aac199d63045a1abe55f6cf10ed28c
msgid "or you can use python client with ``openai`` python package as shown below:"
msgstr ""

#: ../../source/getting_started/quickstart.rst:143
#: e01f689f224b471fbeb116e927c22705
msgid "Next Step"
msgstr ""

#: ../../source/getting_started/quickstart.rst:145
#: 923a25580d8f4525a8d9c4b7fc71c4e7
msgid "Now, you can have fun with Qwen models. Would love to know more about its usages? Feel free to check other documents in this documentation."
msgstr ""
