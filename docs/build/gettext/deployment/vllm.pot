# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2024, Qwen Team
# This file is distributed under the same license as the Qwen package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: Qwen \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2024-03-18 18:18+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: ../../source/deployment/vllm.rst:2
#: f57e127fe7ae4f61b0485210f734c1a4
msgid "vLLM"
msgstr ""

#: ../../source/deployment/vllm.rst:4
#: 49966d0af9c847c6b91fff3c39035807
msgid "We recommend you trying with `vLLM <https://github.com/vllm-project/vllm>`__ for your deployement of Qwen. It is simple to use, and it is fast with state-of-the-art serving throughtput, efficienct management of attention key value memory with PagedAttention, continuous batching of input requests, optimized CUDA kernels, etc. To learn more about vLLM, please refer to the `paper <https://arxiv.org/abs/2309.06180>`__ and `documentation <https://vllm.readthedocs.io/>`__."
msgstr ""

#: ../../source/deployment/vllm.rst:14
#: a23decbe9e8b4920b6d140a6fd775d1f
msgid "Installation"
msgstr ""

#: ../../source/deployment/vllm.rst:16
#: 7032ba75bbf74607afc925dbfb1c1f6e
msgid "By default, you can install ``vLLM`` by pip: ``pip install vLLM>=0.3.0``, but if you are using CUDA 11.8, check the note in the official document for installation (`link <https://docs.vllm.ai/en/latest/getting_started/installation.html>`__) for some help. We also advise you to install ray by ``pip install ray`` for distributed serving."
msgstr ""

#: ../../source/deployment/vllm.rst:24
#: ece6188e288648dca8faac6a68bb2223
msgid "Offline Batched Inference"
msgstr ""

#: ../../source/deployment/vllm.rst:26
#: aba36bd11919485f9b4ba9242d9d635c
msgid "Models supported by Qwen2 codes, e.g., Qwen1.5, are supported by vLLM. The simplest usage of vLLM is offline batched inference as demonstrated below."
msgstr ""

#: ../../source/deployment/vllm.rst:67
#: c634fde321cf46b6bc8a5a1febe0349a
msgid "OpenAI-API Compatible API Service"
msgstr ""

#: ../../source/deployment/vllm.rst:69
#: 8d30e0ba2c764f32a8fe0c6f15a37249
msgid "It is easy to build an OpenAI-API compatible API service with vLLM, which can be deployed as a server that implements OpenAI API protocol. By default, it starts the server at ``http://localhost:8000``. You can specify the address with ``--host`` and ``--port`` arguments. Run the command as shown below:"
msgstr ""

#: ../../source/deployment/vllm.rst:80
#: 4c776952e1c34dcd86581812f20daaf4
msgid "You donâ€™t need to worry about chat template as it by default uses the chat template provided by the tokenizer."
msgstr ""

#: ../../source/deployment/vllm.rst:83
#: f33b51413c7240fe97ebe8cf1f575145
msgid "Then, you can use the `create chat interface <https://platform.openai.com/docs/api-reference/chat/completions/create>`__ to communicate with Qwen:"
msgstr ""

#: ../../source/deployment/vllm.rst:97
#: 1699b17687094892a0c0d45517c5766e
msgid "or you can use python client with ``openai`` python package as shown below:"
msgstr ""

#: ../../source/deployment/vllm.rst:122
#: 44b1fd0ea8864748b161313cf06534e2
msgid "Multi-GPU Distributred Serving"
msgstr ""

#: ../../source/deployment/vllm.rst:124
#: 9774426374c64fb181906a564c3bb70a
msgid "To scale up your serving throughputs, distributed serving helps you by leveraging more GPU devices. Besides, for large models like ``Qwen1.5-72B-Chat``, it is impossible to serve it on a single GPU. Here, we demonstrate how to run ``Qwen1.5-72B-Chat`` with tensor parallelism just by passing in the argument ``tensor_parallel_size``:"
msgstr ""

#: ../../source/deployment/vllm.rst:135
#: 93556310235d4cb1be96279eccc3b7ee
msgid "You can run multi-GPU serving by passing in the argument ``--tensor-parallel-size``:"
msgstr ""

#: ../../source/deployment/vllm.rst:145
#: bdc7e9c09ec748d787bcab15e7752016
msgid "Serving Quantized Models"
msgstr ""

#: ../../source/deployment/vllm.rst:147
#: e4a1efdf0464423da23671b01345e0f3
msgid "vLLM supports different types of quantized models, including AWQ, GPTQ, SqueezeLLM, etc. Here we show how to deploy AWQ and GPTQ models. The usage is almost the same as above except for an additional argument for quantization. For example, to run an AWQ model. e.g., ``Qwen1.5-7B-Chat-AWQ``:"
msgstr ""

#: ../../source/deployment/vllm.rst:158
#: 223b61a5cfe94eee8d2cfd0ceed52072
msgid "or GPTQ models like ``Qwen1.5-7B-Chat-GPTQ-Int8``:"
msgstr ""

#: ../../source/deployment/vllm.rst:164
#: 0688d081d1fd4bf998f952f83ee7f9c5
msgid "Similarly, you can run serving adding the argument ``--quantization`` as shown below:"
msgstr ""

#: ../../source/deployment/vllm.rst:173
#: 63b570cfc37349b58c3bed40500bf71c
msgid "or"
msgstr ""

#: ../../source/deployment/vllm.rst:181
#: af188a075de14ffdb77df6e547a64aaf
msgid "Additionally, vLLM supports the combination of AWQ or GPTQ models with KV cache quantization, namely FP8 E5M2 KV Cache. For example:"
msgstr ""

#: ../../source/deployment/vllm.rst:196
#: c1b06cd197434f6fb1c89d8bfbbd8205
msgid "Troubleshooting"
msgstr ""

#: ../../source/deployment/vllm.rst:198
#: 47500630d0db4d058f99d7bc81e51e84
msgid "You may encounter OOM issues that are pretty annoying. We recommend two arguments for you to make some fix. The first one is ``--max-model-len``. Our provided default ``max_postiion_embedding`` is ``32768`` and thus the maximum length for the serving is also this value, leading to higher requirements of memory. Reducing it to a proper length for yourself often helps with the OOM issue. Another argument you can pay attention to is ``--gpu-memory-utilization``. By default it is ``0.9`` and you can level it up to tackle the OOM problem. This is also why you find a vLLM service always takes so much memory."
msgstr ""
