[instance_hyperparameter]
num_attention_heads = 40
num_key_value_heads = 40
hidden_size = 5120
eos_token_id = 151645
intermediate_size = 13696
vocab_size = 152064
model_name = qwen1.5
modelType = fp16
num_layers = 40
kv_cache_type = 1 ; fp16 refer to DATATYPE in include/core/context.h
embedding_use_postnorm = false ; use postnorm if true else no norm
mha_use_postnorm = false ; use postnorm if true else pre norm
mlp_use_postnorm = false ; use postnorm if true else pre norm
lmhead_use_prenorm = true ; use pre norm if true else no norm
mha_use_residual_add = true
use_rope = true ; use rope or alibi
mlp_use_residual_add = true
residual_has_coeff = false ; if true, coeff will be pow(num_layers * 2), 0.5) else 1.0
rmsnorm = true ; if true use rmsnorm else use layernorm
mlp_activation_gelu = false ; if true use gelu else use swinglu
ropeType = 7 ; topsopRotaryEmbeddingMode_t in include/topstransformer/topsllm/topsop_llm_ops.h
ropeRatio = 1.0 ; model specific, not recommended to change
inLayerNamePrefix = model.layers. ; weight name prefix befor layer-id

[embedding]
wordEmbeddingWeight = model.embed_tokens.weight
wordEmbeddingPostNormWeight = no_wordEmbeddingPostNormWeight
wordEmbeddingPostNormBias = no_wordEmbeddingPostNormBias
embedding_table_type = 1 ; fp16 refer to DATATYPE in include/core/context.h
embedding_norm_type = 1 ; fp16 refer to DATATYPE in include/core/context.h
embedding_out_type = 1 ; fp16 refer to DATATYPE in include/core/context.h

[self_attention]
# all mha weight name are postfix after layer-id
qWeightsName = .self_attn.q_proj.weight
qBiasName = .self_attn.q_proj.bias
qScaleName = no_query_scale
kWeightsName = .self_attn.k_proj.weight
kBiasName = .self_attn.k_proj.bias
kScaleName = no_key_scale
vWeightsName = .self_attn.v_proj.weight
vBiasName = .self_attn.v_proj.bias
vScaleName = no_value_scale
conxtWeightsName = .self_attn.o_proj.weight
conxtBiasName = .self_attn.o_proj.bias
conxtScaleName = no_context_scale
attnNormName = .input_layernorm.weight
attnNormBiasName = no_attnNormBiasName
attn_dot_weight_type = 1 ; fp16 refer to DATATYPE in include/core/context.h
attn_dot_scale_type = 1 ; fp16 refer to DATATYPE in include/core/context.h
attn_dot_bias_type = 1 ; fp16 refer to DATATYPE in include/core/context.h
attn_norm_type = 1 ; fp16 refer to DATATYPE in include/core/context.h
attn_out_type = 1 ; fp16 refer to DATATYPE in include/core/context.h
freqName = inv_freq
freqInEveryLayer = false
freqShape = 64
freqType = 0 ; fp32 refer to DATATYPE in include/core/context.h
multi_query_attention = false ; use multi-query attention if true else use multi-head attention

[mlp]
# all mlp weight name are postfix after layer-id
mlpWeightsOneName = .mlp.gateup_proj.weight
mlpBiasOneName = .mlp.gateup_proj.bias
mlpScaleOneName = no_mlp_dot1_scale
mlpWeightsTwoName = .mlp.down_proj.weight
mlpBiasTwoName = .mlp.down_proj.bias
mlpScaleTwoName = no_mlp_dot2_scale
mlpNormName = .post_attention_layernorm.weight
mlpNormBiasName = no_mlpNormBiasName
mlp_dot_weight_type = 1 ; fp16 refer to DATATYPE in include/core/context.h
mlp_dot_scale_type = 1 ; fp16 refer to DATATYPE in include/core/context.h
mlp_dot_bias_type = 1 ; fp16 refer to DATATYPE in include/core/context.h
mlp_norm_type = 1 ; fp16 refer to DATATYPE in include/core/context.h
mlp_out_type = 1 ; fp16 refer to DATATYPE in include/core/context.h
mlpUseNextLayerNorm = false
lastMLPNormName = no_lastMLPNormName
lastMLPNormBiasName = no_lastMLPNormBiasName

[lm_head]
lmheadWeightsName = lm_head.weight
lmheadScaleName = no_lmhead_scale
lmheadNormName = model.norm.weight
lmheadNormBiasName = no_lmheadNormBiasName
lmhead_dot_weight_type = 1 ; fp16 refer to DATATYPE in include/core/context.h
lmhead_dot_scale_type = 1 ; fp16 refer to DATATYPE in include/core/context.h
lmhead_norm_type = 1 ; fp16 refer to DATATYPE in include/core/context.h
logits_out_type = 0 ; fp32 refer to DATATYPE in include/core/context.h

[sample_parameters]
seed = 1 ; random seed
top_p = 0.8
top_k = -1
temperature = 0
frequency_penalty = 0.0
presence_penalty = 0.0
repetition_penalty = 1.05

[distributed]
distributed_type = 2
embedding_use_tp = true
lmhead_use_tp = true